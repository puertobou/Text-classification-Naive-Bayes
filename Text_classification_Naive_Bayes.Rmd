---
title: "Text classification with Naive Bayes"
author: "Rosa Puerto Bou"
date: "16/02/2021"
output:
  word_document: default
  pdf_document: default
---



# Text classification exercise using the Naive Bayes Classifier.

In this report, the Naïve Bayes Classifiers is used in order to classify text. In concrete, the data set chosen contains some news articles from $New$ $York$ $times$ and $Times$ $of$ $India$. The main objective is to classify them depending on their theme, if they talk about financial frauds or only talk about complementary subjects.

##Data set chosen

The data set is from Kaggle (URL:https://www.kaggle.com/bitswazsky/financial-fraud-and-nonfraud-related-datasets) and it needs some transformations before becoming useful. Kaggle supply the data in two CSV files depending on the class of each article. Therefore, we need to merge both CSV and add an element called $class$ that allows us to identify if the them of the article  is fraud or not. Moreover, this data set contains the title and the summary of each article as separated elements. With the objective to use all the possible information, both elements are merged in a single one called $text$. Lastly, this data set contains the $URL$ of each article but in this case, this information is useless, so it will be removed in order to have a cleaner data set.


```{r preparation}
#Read csv
fraud <- read.csv("fraud.csv",sep=",")
non_fraud<-read.csv("nonfraud.csv",sep=",")

#Write class column
fraud$class<-"1"
non_fraud$class<-"0"

#Convert list to df in order to merge both df
fraud<-as.data.frame(fraud)
non_fraud<-as.data.frame(non_fraud)
data<-merge(x=fraud,y=non_fraud,all=TRUE)

#Remove the url 
data["url"]<-NULL

#Merge title and summary in order to have all the useful text together
text<-paste(data$title,data$summary)
data$text<-text

#Explore elements
names(data)
#Data is now ready to use
```
## Clean data 

The original texts are the following one:

```{r clean1,warning = FALSE}
library(tm)
corpus <- Corpus(VectorSource(data$text))
inspect(corpus[1:3])
```
The first step is convert all the texts into lower cases:
```{r clean2,warning = FALSE}
#Put all in lower cases
corpus<-tm_map(corpus,tolower)
inspect(corpus[1:3])
```

In this step, the numbers are removed too:
```{r clean3,warning = FALSE}
#Remove the numbers
corpus<-tm_map(corpus,removeNumbers)
inspect(corpus[1:3])
```
It is important to remove punctuation,stopwords and excess white space because make the identification of words more difficult. 

```{r clean4,warning = FALSE}

#Remove punctuation
corpus<-tm_map(corpus,removePunctuation)
#Remove stop words
corpus<-tm_map(corpus,removeWords,stopwords("english"))
#Remove the excess white space
corpus<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:3])

```
After working with the data set, a problem has been detected. In particular, the sequence $â€™$ appears in the texts several times. After check some information source, it has been possible to verify that this sequence appeared because of encoding problem and this sequence should represent the apostrophe in the encoding UTF-8.So, to solve it, we use $gsub$ that allows to replace all matches of the specified string to the  desired value.
```{r apostrophe,warning=FALSE}
#Remove apostrophes
inspect(corpus[19])#|â€˜|â€œ|â€¦|â€“|/â€[[:cntrl:]]/
corpus<-tm_map(corpus, gsub, pattern = "â€™|â€˜|$_", replacement = "")
#Removing that string
inspect(corpus[19])
```

## Produce a word cloud
```{r cloud,warning = FALSE,warning=FALSE}
library(wordcloud)
#Fraud class
fraud_indices<-which(data$class == "1")
wordcloud(corpus[fraud_indices], min.freq=80, scale=c(3,.5))
#Non fraud class
non_fraud_indices<-which(data$class == "0")
wordcloud(corpus[non_fraud_indices], min.freq=80, scale=c(3,.5))
```
With this two word clouds, it could be seen that those articles that talk about financial fraud have words like 'fraud','bank','charges','guilty','accused','prosecutors', the majority of them related to the finance field and trials. However, the other word cloud contains more general words without any apparent relationship like 'pollution', 'coronavirus', 'trump'. It should be noted that in this second figure appears some words like 'guilty', 'police', or 'court' that seems to be more related to the financial fraud articles, but it is essential to outline that the other kind of articles can talk about finances only or non-financial related crimes and frauds.

## Divide data into training and test set

This data set contains 5000 instances. The 75% of them (3750 instances) will be in the training set and the rest ones (1250 instances) in the test set.

```{r cloud2,warning = FALSE}
data_matrix<- DocumentTermMatrix(corpus)
inspect(data_matrix[1:4, 30:35])
data_matrix_train <- data_matrix[1:3750,]
data_matrix_test <- data_matrix[3751:5000,]

```
## Naïve Bayes classifier

The final objective of this step is to create a Naïve Bayes classifier. First of all, we identify those words that appears in the text al least 5 times:

```{r frequency, warning=FALSE}
fivefreq_data <- findFreqTerms(data_matrix_train, 5)
corpus_train <- corpus[1:3750]
corpus_test <- corpus[3751:5000]
data_matrix_train  <- DocumentTermMatrix(corpus_train,
control=list(dictionary = fivefreq_data))
data_matrix_test <-DocumentTermMatrix(corpus_test,control=list(dictionary = fivefreq_data))

```
The result is the following one: 
```{r frequency3, warning=FALSE}
inspect(data_matrix_train[1:6,7:16])
```
The following step is to convert the count information into $Yes$ and $No$:
```{r convert,warning=FALSE}
convert_function <- function(x){
y <- ifelse(x > 0, 1,0)
y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
}
data_matrix_train <- apply(data_matrix_train, 2, convert_function)
data_matrix_test <- apply(data_matrix_test, 2, convert_function)
```
The result of that function is the following one:
```{r conver2, warning=FALSE}
data_matrix_train[1:6,7:16]
```
At this point,the Naïve Bayes classifier is created:
```{r naive,warning:FALSE}
library(e1071)
#Labels
data_train<- data[1:3750,]
data_test<-data[3751:5000,]
NB_classifier <- naiveBayes(data_matrix_train, as.factor(data_train$class))

```

Now, the performance is evaluated using the test set:
```{r test}
predictions <- predict(NB_classifier, newdata=data_matrix_test)
```

In order to analyze better the performance, a table with the classifications is presented: 
```{r table}
table(predictions, as.factor(data_test$class))
```
This news filter correctly classifies 79.80% of the non fraud themed articles and 80% of the fraud themed articles. In conclusion, it could be said that it is not a bad filter but its performance could be improved because the percentage of well-classified could be greater.

Finally, the classifier will be runned with Laplacian smoothing:

```{r laplacian }
NB_classifier_Lap<- naiveBayes(data_matrix_train, as.factor(data_train$class),laplace = 1)
class(NB_classifier_Lap)

```
Now, we will evaluated the performance using the test set : 
```{r laplaciantest}
predictions_Lap <- predict(NB_classifier_Lap, newdata=data_matrix_test)
table(predictions_Lap, as.factor(data_test$class))
```
In this case, the filter correctly classifies 78,34% of non fraud themed articles and 81,5% of fraud themed articles. It can be seen that with Laplacian smoothing, the classifier works similar to without it, it has a little improvement classifying the fraud themes but it gets worse with the non fraud themes.
